{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autor: **AiordÄƒchioaei Marius\t| **Assistant: **Suciu Mihai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents  {-}\n",
    "\n",
    "1. Intro\n",
    "\n",
    "    1. Abstract\n",
    "    \n",
    "    2. Motivation\n",
    "\n",
    "2. Core concepts\n",
    "\n",
    "    1. Genetic Programming\n",
    "    \n",
    "    2. Artificial Neural Networks\n",
    "    \n",
    "    3. Integration\n",
    "    \n",
    "    4. Classical Problems\n",
    "\n",
    "3. Aplicability\n",
    "\n",
    "4. Feasability and Impact Analysis\n",
    "\n",
    "    1. Metrics and testing\n",
    "    \n",
    "    2. Observations\n",
    "    \n",
    "5. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "With the Artificial Neural Networks becoming more and more popular and used across all domains, a common problem with designing an eficient one is the tuning of the so-called \"Hyperparameters\". \n",
    "\n",
    "An empirical aproach that addresses this problem is through manual trial and error. This aproach is (most of the time) inefficient and over the hand in almost all the possible ways.\n",
    "Another aproach is doing R&D (Research and Development) on other scientifical publications and finding problems simmilar to ours' that have been solved.\n",
    "\n",
    "The aim of this paper is to demonstrate the feasibility of using a third aproach, automating the process of tweaking the parameters using methods such as Evolutionary Algorithms to find the right Neural Network, fit for a specific domain problem.\n",
    "\n",
    "The process of finding the right Evolutionary Algorithm was all trial and error, with each step documented. Based on some ad-hoc metrics, the algorithm could be brought to a state in which is better for the specific task that are performing.\n",
    "\n",
    "Keywords: \n",
    "\n",
    "Evolutionary algorithm, genetic algorithm, genetic programming, artificial neural network, hyperparameter, brute-force, fitness, natural selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concept is nothing new. It was researched before and proved to be efficient, and the idea of automating a trial-and-error job can cross anyone's mind at some point.\n",
    "\n",
    "The work will foscus more on the Evolutionary part rather than on the Neural Network part.<br>\n",
    "It will also contain a detailed feasibility study of the found solution and testing it with some classical Machine Learning problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core concepts\n",
    "\n",
    "In the following lines we are going to explain the two concepts/techniques used in our case study, as well as the integration between the two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genetic programming\n",
    "<hr/>\n",
    "Genetic programming and Evolutionary Algorithms reffer to a technique of solving problems which solutions are in a wide space of possible solutions. Imagine a solution space which grows exponentially-proportional with the size of the input data.\n",
    "\n",
    "[4] Classical deterministic algorithms are a possible solution to this. Any deterministic algorithm is presumed to do the transition between the current state and the following one in a predictible (or deterministic) manner.<br/>\n",
    "Deterministic algorithms will solve this kinds of problems, but the issue is that their time complexity is unreasonably high, or even <b>exponetialy</b>. So for large input sizes, these sort of algorithms will not scale (in terms of time or space efficiency) on conventional semiconductor machines.\n",
    "\n",
    "For solving our problem, we will focus on intelligent algorhitms.\n",
    "\n",
    "Genetic programming is a method of finding solution inspired from nature, more exactly from the real evolution. <br/>\n",
    "[3] The basic idea of having individuals in population that can mix their genes and get evaluated based on certain characteristics, seems like a good solution for our problem. The brain itself was created in the same manner.<br/>\n",
    "\n",
    "The very general picture of the concept is expressed in the following pseudo-code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Begin\n",
    "    Initialize population with random individuals\n",
    "    Evaluate each individual from the initial population\n",
    "    While ( Certain stop condition )\n",
    "        Select and crossover parents;\n",
    "        Mutate the resulting sons;\n",
    "        Evaluate the new individuals;\n",
    "        Select survivors\n",
    "    End_While\n",
    "End\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very important characteristic to some of the above-mentioned operations (natural selection, mutation) is that they are <b>stochastic</b> which means that in their way of determining the result, they make choices based on randomness or chance. So we know for sure that EAs don't fall under the deterministic algorithm category.\n",
    "\n",
    "In our case study, the individuals are considered to be the Neural Networks that we are trying to evolve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] Components of Evolutionary Algorithms\n",
    "\n",
    " In the following line we are going to describe the basic components which make up an EA.\n",
    "\n",
    "* Representation (how the individuals are encoded in our algorithm)\n",
    "* Fitness measurement method\n",
    "* Population\n",
    "* Parent selection strategy\n",
    "* Crossover \n",
    "* Mutation\n",
    "* Replacement mechanism\n",
    "\n",
    "#### Representation {-}\n",
    "\n",
    "\n",
    "Usually the first step in implementing our EA is choosing a representation. We need to map or translate the objects in the real world space to our defined solution space. The terminology for the objects in the real world space is **phenotypes** and the one for the objects in our solution space (EA space) is **genotypes**.\n",
    "\n",
    "As an example, consider the following problem: Given an undirected graph, find the path with the most nodes. This problem is part of the NP-Complete problem class (which we are not going to prove now). The individual in our EA is the path (a sequence of linked nodes). The phenotype is the path itself. One genotype can be a binary number which has 1 on pozition j for j node is in sequence, 0 otherwise (even though this genotype doens't conside the order of the nodes, but this might not be a concern in our algorithm). Another genotype can be a simple list of ordered nodes.\n",
    "\n",
    "#### Fitness {-}\n",
    "\n",
    "\n",
    "The role of the fitness function is to objectively measure how close is one individual to the desired solution. Based on this metric we can decide if one individual is fit enough to continue in our population or is not fit enought and needs to be replaced by better individuals (natural selection)\n",
    "\n",
    "#### Population {-}\n",
    "\n",
    "\n",
    "The population is the set of individuals at some point (any point) in our EA. It can be seen as a set of genotypes of many generations. The genotypes themselfs is not evloving in any way, but the population is, through the creation of the new and more fit genotypes.\n",
    "\n",
    "An important metric here is the diversity of the population which measures how many different solution we have. \n",
    "To maintain diversity we need to give weaker genotypes a chance to reporoduce, because they might have valuable genetic material which combined with the right counterparty, might give us a fit individual. A population which is not diverse, tends to converge with the best fitness and might never give us a fit enough individual. Elitism in a EA (often chosing the best) can damage the diversity. This is the equivalent of a greedy algorithm which at each given step, it chooses only the local optimum.\n",
    "\n",
    "#### Parent selection {-}\n",
    "\n",
    "\n",
    "The parent selection mechanism is the rule by which we allow individuals to bring offsprings into the population by combining their genes (becoming parents). In this part we get to choose parents, mosty based on their quality (fitness) but also based on some stochastic elements.\n",
    "\n",
    "The reason that we are introducing stochastic elements here, is that we want to avoid a homogenous population by giving the unfit individuals the chance to become parents so we can improve the diversity discuseed above.\n",
    "\n",
    "#### Crossover {-}\n",
    "\n",
    "\n",
    "The crossover operation has two genotypes as input and one or two offspring(s) as an output. By mergeing the genes of the input individuals, we are able to create the offspring which inherits traits from the parents and probabilistically has a higher fitness.\n",
    "\n",
    "In the designing of our EA, it is crucial to choose a good crossover mechanism.\n",
    "\n",
    "#### Mutation {-}\n",
    "\n",
    "\n",
    "Mutation naturally happenes in the real world. But in EAs are a method of keeping the population diverse. When is born through crossover, the genotype has a pre-defined chance to become mutated. It means that the features inherited from the parents are slightly changed.\n",
    "\n",
    "#### Replacement {-}\n",
    "\n",
    "\n",
    "Replacement distinguishes individual based on their fitness and age (with stochastic influences). It is used to eliminate the individuals that haven't proved themselfs to have any value to the evolution process, or just the genotypes that are unfortunately chosen by the stochastic elements (which translates into environmental hazard).\n",
    "This is mostly one in the same step as the Parent selection part, when the individuals that are not selected for reproduction, are disposed from the populaiton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks\n",
    "<hr/>\n",
    "\n",
    "Artificial Neural Networks (ANN) is an approach to the Supervized Learning problems that are more and more used in multiple industries. The method inspires heavily from how our brain works, even though it is an overly-simplified process.\n",
    "\n",
    "### Seen as Black-Box {-}\n",
    "\n",
    "[2] An ANN, seen as a black-box entity, is composed of 3 parts: input layer, hidden layers and output layer.\n",
    "So here we have to deal with a layered architecture in which each layer is composed of neurons. \n",
    "\n",
    "![General idea of ANN](gfx/ann.jpg \"General idead of ANN\")\n",
    "As observed in the picture above, each neuron from one layer linked to all the neurons in the layer after (even though, is not really a constraint to be linked to all neurons).\n",
    "\n",
    "The supervized learning part means that we supervize the learning process by giving feedback to the performance of our ANN for each test. We tell to our ANN when is wrong, and more importantly, how much it is wrong, so it can learn not to output the same wrong value again (through methods that we are going to describe later).\n",
    "\n",
    "So, in our learning phase of the ANN, we are going to need training data, which consists of input values, and output values that are considered to be correct. Our ANN is going to learn to handle new inputs, and output values that are the desired ones, very much like how the brain learns.\n",
    "\n",
    "The quality of the output relies heavily on many factors, like:\n",
    "\n",
    "* The architecture of the ANN used. (how many hidden layers we use, the size of each layer, activation functions, etc.)\n",
    "* The quality of the training data. (training data needs to explore enough of the solution space if we want correct outputs)\n",
    "\n",
    "### [2] Seen as White-Box  {-}\n",
    "\n",
    "The state of an ANN consists of some values called weights and biases. Learning process comes down to finding the right values for the weights and biases.\n",
    "\n",
    "![Neuron](gfx/neuron.png \"General idead of ANN\")\n",
    "For each neuron, we have its input values from the previous layer (in case of the input layer, we will have only one single input value). \n",
    "We have weight values (noted as w) for each link to the previous layer. It represents the inportance we give to the input of the predecessor neuron.\n",
    "\n",
    "Altogether, the value outputed by our neuron will be\n",
    "$$ x_{1}\\cdot w_{1} + x_{2}\\cdot w_{2} + x_{3}\\cdot w_{3} + b $$\n",
    "Where b is the bias\n",
    "\n",
    "Dependending on the type of the neutron, the output can be projected through different mathematical functions, such as sigmoid, softmax and linear rectifier. Such function is called activation function and also has impact on the overall performance of the ANN.\n",
    "\n",
    "The adjustment of the weight and bias values is done in the learning phase, through a method called error back propagation.\n",
    "First, it is calculated how far (in distance) is the output from the actual solution, and then the error is propagated back to the layers it came from. The weights and biases are recalculated in such manner that on the next evaluation with the same input, the outputed error will be smaller.\n",
    "\n",
    "For the error back propagation, a very used method is gradient descend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration\n",
    "\n",
    "The two concepts described above go well hand in hand. We can define our phenotype as an ANN which is fit for having a better accuracy after a training session.\n",
    "\n",
    "This way we are will be able to find the right ANN architecture for our problem, without much effort.\n",
    "\n",
    "We need to define our genotype. As mentioned in the abstract, our genotype will consist of the ANN's hyperparameters. A hyperparameter is that value which is configured before the process of learning begins.\n",
    "\n",
    "The genotype can consider:\n",
    "\n",
    "* Number of the hidden layers\n",
    "* Size of each layer\n",
    "* Type of each layer along with activation function\n",
    "* Initial weights\n",
    "* Initial bias\n",
    "\n",
    "In the current state of the implementation, our genotype will regard only the number of hidden layers and the size of each hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical Problems\n",
    "\n",
    "Here we are going to mention the Machine Learning problem on which we tested our algorithm.\n",
    "\n",
    "#### [2] Digit Recognition {-}\n",
    "\n",
    "This is the \"hello world\" of Machine Learning. In the testing, we used the [MNIST](http://yann.lecun.com/exdb/mnist/) database. It provides 60,000 train instances along with 10,000 test instances. Each train instance is a picture of one of the 10 digits, in a bitmap format. The size of each bitmap is 28x28 pixels and the chromatic range of each pixel is one byte monochrome (so a color value ranges between 0-255).\n",
    "\n",
    "The way we are feeding this onto our ANN is: we have an input layer of 28\\*28 = 784 neurons, one for each pixel.\n",
    "This is a classification problem, so the output layer have one neuron for each class, in our case 10 neurons.\n",
    "Our ANN will evaluate how likely (in terms of chance) is for our input to be classified as each class. So if we sum the outputs of each neuron of the last layer, we need to get 1. To achieve the translation between the 0-255 ranged input values and probabilistic subunitary values we need to set the activation funciton of the last layer to softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicability\n",
    "\n",
    "As the practical part, I used for my case study different technologies and libraries:\n",
    "\n",
    "* All the code is implemented in Python.\n",
    "* For the ANN API I used Keras along with TensorFlow backend and for scalability concerns I used Tensor GPU to make tensor use specialized hardware for the learning workload.\n",
    "* For ploting certain metrics I logged parts of the algorithm and then extracted the information and plotted it using Matplotlib. \n",
    "\n",
    "To create a clear architecture of the code, I wrapped all the behaviour of the Keras library in some decorator classes so that I have a clear API which speaks Evolutive Algorithm terminology. For example, I wrapped the ANN which extends \"Individual\" class in such manner that the ANN is contracted to implement EA operations like crossover or mutate.\n",
    "\n",
    "## Code architecture\n",
    "\n",
    "![Code architecture](gfx/Architecture.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feasability and Impact Analysis\n",
    "</hr>\n",
    "In the implementation phase, I decided to use an iterative system. In each iteration, the algorithm was tested, observations were made based on some metrics, and improvement ideas were planned for implementation in the next iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and testing\n",
    "\n",
    "For testing the algorithm, we used here the character recognition problem mentioned above, with MNIST database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 1 {-}\n",
    "\n",
    "In the initial implememtation, we have the natural selection funciton, which has as input a list of individuals, sorted descendingly by their measured fitness. For each 2 individuals indexed i and j, we calculate their chance to reproduce:\n",
    "\n",
    "$$\\frac{i \\cdot j}{{(n - 1)}^{2}}$$\n",
    "\n",
    "A selection for replacement was also implemented, which consideres a probability, which is proportional to the fitness of the indivudual, and also the age of the individual (generation count).\n",
    "For each individual i, his chance to be replaced is:\n",
    "\n",
    "$$\\frac{a \\cdot (1 - \\frac{i}{n})}{E}$$\n",
    "\n",
    "Unde E = life expectancy and n = total number of individuals\n",
    "\n",
    "### Parameters {-}\n",
    "Life expectancy: 3\n",
    "\n",
    "### Crossover {-}\n",
    "The crossover of the neural nets was done through the combination of the hidden layers.\n",
    "\n",
    "In the case that the crossover parties have different numbers of layers, a random sampling happenes on the net with more layers. The size of the sampling is the number of layers of the one with fewer layers (lets call it A and the counterparty B). The bad thing here is that the genes of A can be proportionally dominant compared to B, as the layers of B, that are not included in the sampling, are discarded in the process.\n",
    "\n",
    "### Fitness {-}\n",
    "\n",
    "The calculation of the fitness is (currently) based only on the accuracy of the neural network.\n",
    "\n",
    "### Metrics {-}\n",
    "\n",
    "* Fitness trend\n",
    "\n",
    "<hr/>\n",
    "![Fitness trend](gfx/fitness-trend-tne-1.png)\n",
    "\n",
    "* Generation size (Blue - number of individuals in the population, Green - maximum fitness) \n",
    "\n",
    "<hr/>\n",
    "![Generation size](gfx/generation-size-tne-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration 2 {-}\n",
    "\n",
    "### Parameters {-}\n",
    "\n",
    "Life expectancy: 1.3\n",
    "\n",
    "### Crossover {-}\n",
    "\n",
    "Unchanged.\n",
    "\n",
    "### Fitness {-}\n",
    "\n",
    "Unchanged.\n",
    "\n",
    "### Metrics {-}\n",
    "* Fitness trend\n",
    "\n",
    "<hr/>\n",
    "![Fitness trend](gfx/fitness-trend-tne-2.png)\n",
    "\n",
    "* Generation size (Blue - number of individuals in the population, Green - maximum fitness, Dark blue - average fitness) \n",
    "\n",
    "We are now measuring also the average fitness of the population\n",
    "\n",
    "<hr/>\n",
    "![Fitness trend](gfx/generation-size-tne-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "The main observations made throughout the first two iterations focuses on weak points of the algorithm:\n",
    "\n",
    "* One major weakness is that the populaiton grows quickly.\n",
    "\n",
    "\n",
    "* This simply doesn't scale, because finding a good solution ot our problem, we need to grow a certain amount of generations, approximately 10. To grow 4 generations, the program ran for about ~12 hours. The conclusion drew by Generation Size metric, this approach doesn't scale. To perform a crossover between the genotypes of the generation 4 would take an unreasonably huge amount of time.\n",
    "\n",
    "\n",
    "* Another observations here is that the average fitness converges to a value. This means that the genotypes become homogenous. There is not enough new genetical material and this will not help us raise the maximum fitness either.\n",
    "\n",
    "\n",
    "* We can see how the maximum fitness increases, but not signtificanlty enough to justify the need of such algorithms to find a better solution. A significant fitness increase would be achieved by growing a larger number of generations.\n",
    "\n",
    "\n",
    "* The crossover function can be also improved. When performing crossover, we can consider all the genes of the individuals, without too much domination from one party.\n",
    "\n",
    "\n",
    "* We can consider other hyperparameters in finding the optimal solution, like initial weights and biases.\n",
    "\n",
    "\n",
    "* The fitness function can also consider the physical running time as a quality of a genotype.\n",
    "\n",
    "\n",
    "* [5] For the following iterations, we will use Tournament Selection for selections. This should give us a controllable number of genotypes and more generations to grow.\n",
    "\n",
    "\n",
    "* We can consider trying the same test but for other problems (which can also be more complex) and other trainig data sets.\n",
    "\n",
    "\n",
    "* The initial genotypes can be generated randomly.\n",
    "\n",
    "\n",
    "* The input and the train data can be randomly sampled (k-fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] [Letâ€™s evolve a neural network with a genetic algorithm](https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164)\n",
    "\n",
    "[2] [Using neural nets to recognize handwritten digits](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "\n",
    "[3] [What is an Evolutionary Algorithm](https://www.cs.vu.nl/~gusz/ecbook/Eiben-Smith-Intro2EC-Ch2.pdf)\n",
    "\n",
    "[4] [Computational complexity](https://en.wikipedia.org/wiki/Computational_complexity)\n",
    "\n",
    "[5] [An Evolutionary Approach for Tuning Artificial Neural\n",
    "Network Parameters](https://pdfs.semanticscholar.org/7d7b/84986459ccc1de349342d44fde999c3de8e9.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
